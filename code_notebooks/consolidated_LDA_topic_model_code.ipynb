{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Scrape\n",
    "Use the reddit pushshift API to scrape the raw data needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access reddit posts via the reddit pushshift API ...\n",
    "\n",
    "url = \"https://api.pushshift.io/reddit/search/submission?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'subreddit': 'parenting',\n",
    "    'size': 1_000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url, params)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = raw_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick vis check ...\n",
    "posts[349]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Performa quick test of the timestamp conversion before implementing\n",
    "test = 1230768000\n",
    "datetime.fromtimestamp(test).strftime(\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before running the cell below, update your file path location (see line 17) where you want to save the raw data from your scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# period starting 1 Jan 2015, ending 31 Dec 2019, bi-weekly interval\n",
    "for i in range(1420088400, 1577854799, 1209600):\n",
    "    # corpus = []\n",
    "    params = {\n",
    "        'subreddit': 'parenting',\n",
    "        'size': 1_000,\n",
    "        'after': i\n",
    "    }\n",
    "    res = requests.get(url, params)\n",
    "    raw_data = res.json()\n",
    "    posts = raw_data['data']\n",
    "    text = pd.DataFrame(posts)\n",
    "    text = text[['subreddit', 'created_utc', 'title', 'selftext', 'score']]\n",
    "    \n",
    "    full_text = full_text.append(text)\n",
    "    full_text.to_csv('../data/parenting_posts.csv')\n",
    "    \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget to update the file path in the cell below before reading in data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw data from our .csv file\n",
    "\n",
    "data = pd.read_csv('../data/parenting_posts.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicate records in the data set\n",
    "data['created_utc'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that there are clearly duplicate observations in the data, so we'll want to remove all those that are dupes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the duplicate records from the data set ...\n",
    "deduped_data = data.drop_duplicates(subset='created_utc')\n",
    "deduped_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional deduping check ...\n",
    "deduped_data['created_utc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your data to see if you have the column, \"Unnamed: 0\".  If yes, remove the column to further clean up the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping old index column\n",
    "deduped_data.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any null values that could cause problems with our modeling ...\n",
    "deduped_data['created_utc'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_data = deduped_data[deduped_data['created_utc'].notna()]\n",
    "deduped_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to be able to use tha time stamp informaiton for some exploratory analysis, so we'll want to transform it into a more readable format.  Before we can do that, we need to transform it from a string type to an integer type.  Once we do that, we can transform it as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_data['created_utc'] = deduped_data['created_utc'].astype(int)\n",
    "deduped_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = datetime.fromtimestamp(deduped_data['created_utc'][2]).strftime(\"%Y-%m\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate approach to convert time stamp to more interpretable format ...\n",
    "# def convert(systime):\n",
    "#     return datetime.fromtimestamp(systime).strftime(\"%Y-%m\")\n",
    "# Input/guidance from Teng Mao for this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_data['yr_mo'] = deduped_data['created_utc'].apply(convert)\n",
    "deduped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually checking random values in middle of data ...\n",
    "deduped_data[72000:72500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "I'd like to get some sense of what's happening with this reddit forum, in terms of usage, activity levels, trends, etc.  So we'll look at a few plots to give us a sense of what that activity looks like.  Plese note that this is just a sample of the visual inspection executed, for brevity's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.hist(deduped_data['yr_mo'],\n",
    "         color='purple',\n",
    "         alpha=0.3,\n",
    "         bins=60);\n",
    "plt.xlabel('Volume of Unique \"Parenting\" Posts (reddit.com)\\n 1 Jan 2015 - 31 Jan 2020 \\n Unique Posts in Sample = 102,776',\n",
    "          fontsize=20)\n",
    "blanks = []\n",
    "plt.xticks(ticks=blanks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.xlabel('Vote Scores for Unique \"Parenting\" Posts (reddit.com) \\n 1 Jan 2015 - 31 Jan 2020',\n",
    "           fontsize=20)\n",
    "plt.scatter(deduped_data['yr_mo'], \n",
    "            deduped_data['score'], \n",
    "            marker='D', \n",
    "            edgecolors='purple',\n",
    "            color='teal', \n",
    "            alpha=0.3)\n",
    "blanks = []\n",
    "plt.xticks(ticks=blanks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(deduped_data['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking against raw data for comparison ...\n",
    "plt.plot(data['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplifying the data set name from \"deduped_data\" to \"ddd\"\n",
    "ddd = deduped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test removal of sys text from a single record ...\n",
    "ddd['selftext'][74409]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target text for replacemnt\n",
    "target_words = ['[deleted]', '[removed]']\n",
    "\n",
    "ddd['selftext'].replace(target_words, \"\", inplace=True)\n",
    "ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge title and post (i.e. \"selftext\") content into single text field.\n",
    "# This merged field will be our sole focus for the subsequent topic modeling exercise.\n",
    "ddd = ddd.replace(np.nan, '', regex=True)\n",
    "ddd['all_text'] = ddd['title'] + \" \" + ddd['selftext']\n",
    "ddd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd['all_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your file path for where to save the cleaned data file before running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd.to_csv('../data/working_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing and LDA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various libraries for preprocessing steps ...\n",
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP-related libraries ...\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the parenting posts dataset ...\n",
    "pp = pd.read_csv('../capstone_datasets/working_data.csv')\n",
    "pp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pp.all_text.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ages of children referenced in the posts are materially relevant to our analysis,\n",
    "# so we'll construct a tokenizer that allows us to keep numeric digits.\n",
    "\n",
    "corpus = []\n",
    "def alt_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+|\\d+')\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_to_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visual inspection to ensure we retained the desired information ...\n",
    "corpus[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "Note that this is a technique/tool for reducing words to their root to facilitate NLP analysis and modeling.  For this project, I also experimented with Stemming (an alternate tool to Lemmatizing that does essentially the same thing, albeit a bit more severely, on average).  Because the results were not materially different when modeling with the stemmed vs lemmed vacabularies, we'll present only the lemmatized version here for the sake of brevity.\n",
    "\n",
    "Note that the needed libraries were imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lemmetizer\n",
    "lemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed_corpus = [[lemmer.lemmatize(i) for i in sublist] for sublist in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed_corpus[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[1] == lemmed_corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side by side comparison ...\n",
    "list(zip(corpus[1], lemmed_corpus[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all words in the corpus are lowercase before removing stopwards ...\n",
    "\n",
    "lower_lemmed_corpus = [[x.lower() for x in sublist] for sublist in lemmed_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stopwords\n",
    "# note this may take a minute... or three\n",
    "low_lem_corpus_nosw = [[w for w in sublist if w not in stopwords.words('english')] for sublist in lower_lemmed_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(low_lem_corpus_nosw[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_lem_corpus_nosw[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer() can't take in a list of lists, so converting input data\n",
    "# to single list of words in order to get the features using CVect ...\n",
    "\n",
    "lemset = [\" \".join(doc) for doc in low_lem_corpus_nosw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the lemmetized lists of words in the posts ...\n",
    "# Note that you may want to tune the hyperparamters in the vectorizer via gridsearch,\n",
    "# but that will come at a cost in terms of processing power and time to process\n",
    "\n",
    "vec_lem = CountVectorizer(lowercase=False, \n",
    "                      ngram_range=(1, 2), \n",
    "                      max_df = 0.95, \n",
    "                      min_df = 0.0075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed_features = vec_lem.fit_transform(lemset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_lem = vec_lem.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fitting Initial LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some initial steps to prep for gensim LDA modeling ...\n",
    "# Need to define the index for all the words in the corpus\n",
    "lem_word_index = corpora.Dictionary(low_lem_corpus_nosw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marry the index with the corpus to be used in the model\n",
    "texts = low_lem_corpus_nosw\n",
    "lem_corpus = [lem_word_index.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model ... \n",
    "# initially modeling 24 topics, just as a point of departure\n",
    "# Be prepared to wait when you fit the model as it can take several hours\n",
    "# to run, depending on the size of your data and your hardware capabilities.\n",
    "\n",
    "lda_model_lem_24 = gensim.models.ldamodel.LdaModel(corpus=lem_corpus, \n",
    "                                                   id2word=lem_word_index, \n",
    "                                                   num_topics=24,\n",
    "                                                   random_state=1972,\n",
    "                                                   update_every=1,\n",
    "                                                   chunksize=100,\n",
    "                                                   passes=10,\n",
    "                                                   alpha='auto',\n",
    "                                                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_lem_24.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance: perplexity and coherance scores\n",
    "print('Perplexity: ', lda_model_lem_24.log_perplexity(lem_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's coherence score\n",
    "coherence_ldamodel_lem_24 = CoherenceModel(model=lda_model_lem_24, \n",
    "                                       texts=low_lem_corpus_nosw, \n",
    "                                       dictionary=lem_word_index, \n",
    "                                       coherence='c_v')\n",
    "coherence_lda_lem_24 = coherence_ldamodel_lem_24.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda_lem_24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Visulaization\n",
    "If you are unfamiliar with the pyLDAvis library, it is advisable to spend some time with the documentation as it is a very powerful dimensionality reduction and visualization tool for viewing LDA topic modeling results.  Note that the more separate and defined the clusters are, and the bigger they are, the stronger the model is.  \n",
    "\n",
    "Note also that it can take several minutes or longer to process, depending on the size of your dataset and the particulars of your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook(sort=True)\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_lem_24, lem_corpus, lem_word_index)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is worth pointing out that I switched to the LDA mnodel in scikit learn in order to facilitate leveraging GridsearchCV to facilitate finding the \"best\" number of topics for this corpus.  Please refer to the gridsearch notebook to see that code (but note that all the data wrangling, EDA, and preprocessing in this notebook need to be completed first).  I chose to switch to the Scikit Learn LDA model for this process because it is a bit more stratight foreward and expediant, compared to doing the same thing with the gensim LDA library.  When time permits, it would be interesting to do a side-by-side comparison to see if/how results may vary sa there does appear to be a difference in the results of from the two LDA models.\n",
    "\n",
    "In any case, after gridsearching multiple N's for the number of topics, 15 topcics was among the best parameters from the gridsearched scikit learn LDA model, but plugging back in the the gensim LDA model, 12 seemed to yeild the strongest results, so we'll proceed with those parameters - results can be seen below.  \n",
    "\n",
    "While not displayed here for the sake of notebook readability, I did also run several of the other N's to the gensim LDA model to validate that 12 was, in fact, the better fit, and all those results corroborated that this should be the case.  Just remember that cluster modeling is a relatively imprecise practice and there may not be a single, \"best\" option.  In all cases, the words associated with each topic yielded a pretty clear picture of what each topic would be about.  As N increased, the model fit scores worsened and clusters overlapped more, but there was more granularity in the topics yielded.  That said, if the ultimate goal of the exercise is to yield topics so that new content can be categorized and labeled by a machine learning algorithm, then cleaner separability (i.e. the least overlappping of clusters possible) likely would yield be better results.\n",
    "\n",
    "Since this client project did not require this secondary step/analysis, getting the clusters exactly perfect was less of an issue.  In any case, we'll proceed with the 12-topic example for the duration of the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearching with ScikitLearn LDA model\n",
    "**Note that this section of code is optional**, it is not necessary to run to complete the rest of the notebook.  If you want to skip this section, you can proceed direction the the 12-topic model and continue from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearching the LDA Model\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Parameters\n",
    "search_params = {\n",
    "    'n_components': [10, 12, 15, 19, 30], \n",
    "    'learning_decay': [.5, .7, .9]\n",
    "}\n",
    "\n",
    "# Init the Model\n",
    "lem_lda_gs = LatentDirichletAllocation(n_jobs=-1, \n",
    "                                       random_state=1972, \n",
    "                                       learning_method='online')\n",
    "\n",
    "\n",
    "# We want to gridsearch without any cross validation (for an unsupervised mode)\n",
    "# found the below solution at: https://stackoverflow.com/questions/44636370/scikit-learn-gridsearchcv-without-cross-validation-unsupervised-learning\n",
    "cv_off = [(slice(None), slice(None))]\n",
    "\n",
    "# Init Grid Search Class\n",
    "gscv_model = GridSearchCV(lem_lda_gs, param_grid=search_params, n_jobs=-1, cv=cv_off)\n",
    "\n",
    "# Do the Grid Search\n",
    "# model.fit(data_vectorized)\n",
    "\n",
    "# lem_lda_gs.fit(lemmed_features)\n",
    "gscv_model.fit(lemmed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing from 24 to 12 topics and consilidating the scoring into one cell\n",
    "# this will take some time again ...\n",
    "lda_model_lem_12 = gensim.models.ldamodel.LdaModel(corpus=lem_corpus,\n",
    "                                               id2word=lem_word_index,\n",
    "                                               num_topics=12,\n",
    "                                               random_state=1972,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "\n",
    "# Get topics ...\n",
    "lem_topics_12 = lda_model_lem_12.print_topics()\n",
    "\n",
    "# Calculate and save the perplexity score\n",
    "lem_top_12_perpscore = lda_model_lem_12.log_perplexity(lem_corpus)\n",
    "\n",
    "# CInstantiate the confusion score model\n",
    "coherence_ldamodel_lem_12 = CoherenceModel(model=lda_model_lem_12, \n",
    "                                       texts=low_lem_corpus_nosw, \n",
    "                                       dictionary=lem_word_index, \n",
    "                                       coherence='c_v')\n",
    "\n",
    "# Calculate and save the coherence score\n",
    "coherence_lda_lem_12 = coherence_ldamodel_lem_12.get_coherence()\n",
    "\n",
    "print(f'Perplexity: {lem_top_12_perpscore}\\n Coherence Score: {coherence_lda_lem_12}\\n 12 Lemmed LDA Model Topics:\\n {lem_topics_12}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 12-topic model\n",
    "pyLDAvis.enable_notebook(sort=True)\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_lem_12, lem_corpus, lem_word_index)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the time it takes to process the bulk of the steps outlined above, you may find it desireable to persist the fitted model and pyLDAvis results in order to grately accelerate your ability to demonstrate and review results with others.  Code to achieve those ends is provided below in steps 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Persisting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lem_corpus, open('lem_corpus.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need additional libraries in order to save and reload gensim \"keyed vector\" objects ...\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lem_word_index.save('../capstone_datasets/lem_word_index.index')\n",
    "fname = get_tmpfile(\"lem_word_index.kv\")\n",
    "lem_word_index.save(fname)\n",
    "\n",
    "# word_vectors = KeyedVectors.load(fname, mmap='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test save the 12-topic LDA model ...\n",
    "lda_model_lem_12.save('lda12.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.corpora as corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call pickle and gensim saved objects ...\n",
    "\n",
    "lda12 = LdaModel.load('lda12.model', mmap='r')\n",
    "my_corpus = pickle.load(open('lem_corpus.pickle', 'rb'))\n",
    "my_index = KeyedVectors.load('lem_word_index.index', mmap='r')\n",
    "vis_demo = pickle.load(open('vis_demo.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis_demo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
